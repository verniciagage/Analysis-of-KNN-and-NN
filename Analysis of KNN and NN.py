# -*- coding: utf-8 -*-
"""Automation_and_Robotics_Project (2) (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZgRX1np9kJgQIGhmeogY625HA1cXTloy
"""

# Import the Python libraries
import matplotlib.pyplot as plt
import cv2
import numpy as np
import seaborn as sns

import itertools as it
import os
import glob
import os.path as osp

from sklearn.model_selection import StratifiedKFold

from sklearn import (datasets, metrics,                     
                     model_selection as skms,
                     neighbors, svm)

from sklearn.neural_network import MLPClassifier

import time

import keras

from keras.datasets import fashion_mnist

# don't usually prefer/do direct imports, but so be it ...
from keras.models import Sequential
from keras.layers import (Activation, 
                          Dense, Dropout, Flatten,
                          Conv2D, MaxPooling2D)

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/gdrive')
!ls "/content/gdrive/My Drive/Colab Notebooks/common"
# %cd /content/gdrive/My Drive/Colab Notebooks

# Import the .py file with the functions needed
from utilities import my_show, my_gshow, my_read, my_read_g, my_read_cg

"""Import Data"""

fashion = fashion_mnist.load_data()
# the data, shuffled and split between train and test sets
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()
n_train, n_test = x_train.shape[0], x_test.shape[0]
num_classes = len(set(y_train)) # aka, 10

plt.figure()
plt.imshow(x_train[0])
plt.colorbar()
plt.grid(False)
plt.show()

# Display the first 5 samples of the dataset with their corresponding labels
fig, axes = plt.subplots(1,5)
first_four_wtgt = it.islice(zip(x_train, y_train), 5)
for (image, label), ax in zip(first_four_wtgt, axes):       
    my_gshow(ax, image)
    ax.set_title("True: {}".format(label))

# here we flatten the data 
# (so r,c information is lost - rely on absolute position only)
x_train = x_train.reshape(n_train, -1).astype(np.float32) / 255.0
x_test  = x_test.reshape( n_test, -1).astype(np.float32) / 255.0

flat_shape = x_train[0].shape

print('{} train and {} test examples'.format(n_train, n_test))

# convert class vectors to binary class matrices
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test  = keras.utils.to_categorical(y_test,  num_classes)

y_train

def cm_helper(ax, actual, predicted):
    cm = metrics.confusion_matrix(actual, predicted)
    sns.heatmap(cm, annot=True, fmt='3d', ax=ax)
    ax.set_ylabel('Actual')
    ax.set_xlabel('Predicted')
    
# Provide a seed for reproducibility of the results.
# By providing a seed, the randomly generated numbers will be the same every time
# you run your code. This way you can check/test your code with the same values
# Here, the random seed is 42 but you can select any number, just make sure that
# you use the same every time you run your code
np.random.seed(42)

# Commented out IPython magic to ensure Python compatibility.
# Make sure to provide this line to display figures in the notebook
# %matplotlib inline

"""Neural Network"""

# Neural Network
MLP_model = MLPClassifier()

# Define the set of values that will be evaluated for the hyperparameter
# Here, we are testing 
#nn_param_grid = {'alpha':[0.0001, 0.05], 'activation':['tanh','relu'], 'hidden_layer_sizes': [1, 10]}#, 25, 50, 75, 100, 200]}
nn_param_grid = {'alpha':[0.0001], 'activation':['relu'], 'hidden_layer_sizes': [10]}#, 25, 50, 75, 100, 200]}

# Set the seed for reproducibility
np.random.seed(42)

# Start the timer to check the training time for the model
start_time = time.time()


# fit model
nn_model = skms.GridSearchCV(estimator=MLP_model,  param_grid=nn_param_grid, cv=3, return_train_score=True, verbose=1)
nn_model.fit(x_train, y_train)

end_time = time.time()
total_time = end_time - start_time
print('Time: %.2f seconds' % total_time)

# Make predictions for the entire test set
nn_model_predict = nn_model.predict(x_test)

# See how the model performed
report_nn_model = metrics.classification_report(y_test, nn_model_predict)
print(report_nn_model)

# Display confusion matrix
cm_helper(plt.gca(), y_test.argmax(axis=1), nn_model_predict.argmax(axis=1))

print("NN Confusion Matrix for Best Parameters")
print("Parameters: ", nn_model.best_params_)

# evaluate
#score = nn_model.evaluate(x_test, y_test)
#print('Test loss:', score[0])
#print('Test accuracy:', score[1])

nn_model_score = nn_model.score(x_test, y_test)
print(nn_model_score)

print(nn_model.cv_results_)

# Identifies nn_model nean training error
nn_model_mean_train = np.array(nn_model.cv_results_['mean_train_score'])
print(nn_model_mean_train)

# Identifies nn_model nean testing error
nn_model_mean_test = np.array(nn_model.cv_results_['mean_test_score'])
print(nn_model_mean_test)

"""KNN"""

# Define the set of values that will be evaluated for the hyperparameter
# Here, we are testing 1 - 11
knn_param_grid = {'n_neighbors': [1, 2, 3, 4]}#, 5, 6, 7, 8, 9, 10, 11]}

# StratifiedKfold and gridsearch to find the best hyperparameter k for the KNN classifier
# Set the seed for reproducibility
np.random.seed(42)

# Start the timer to check the training time for the model
start_time = time.time()

# This is using 5-fold cross-validation. It also keeps the training scores
knncv = skms.GridSearchCV(neighbors.KNeighborsClassifier(), param_grid=knn_param_grid, cv=3, return_train_score=True, verbose=1)
#knncv = neighbors.KNeighborsClassifier(n_neighbors = 2)
knncv.fit(x_train, y_train)

end_time = time.time()
total_time = end_time - start_time
print('Time: %.2f seconds' % total_time)

# Make predictions for the entire test set
knncv_predict = knncv.predict(x_test)

# See how the model performed
report_knncv = metrics.classification_report(y_test, knncv_predict)
print(report_knncv)

# Display confusion matrix
cm_helper(plt.gca(), y_test.argmax(axis=1), knncv_predict.argmax(axis=1))

print("Parameters: ", knncv.best_params_)

print(knncv.cv_results_)

knncv_k = np.array([1, 2, 3])#, 4, 5, 6, 7, 8, 9, 10, 11])
print(knncv_k)

# Identifies knn nean training error
knncv_mean_train = np.array(knncv.cv_results_['mean_train_score'])
print(knncv_mean_train)

# Identifies knn nean testing error
knncv_mean_test = np.array(knncv.cv_results_['mean_test_score'])
print(knncv_mean_test)

plt.plot(knncv_k, knncv_mean_train, color='blue')
plt.plot(knncv_k, knncv_mean_test, color='red')
plt.xlabel('Number of Neighbors')
plt.ylabel('Mean Scores')
plt.title('Mean Train and Mean Test Scores vs. Number of Neighbors Tested')
plt.xticks([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])
colors = {'Mean Train Scores':'blue', 'Mean Test Scores':'red'}         
labels = list(colors.keys())
handles = [plt.Rectangle((0,0),1,1, color=colors[label]) for label in labels]
plt.legend(handles, labels);